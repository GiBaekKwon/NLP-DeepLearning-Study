4주차. 어텐션 메커니즘 (Attention Mechanism)
일정: 2026/02/19 16:00~18:00

1. 어텐션 메커니즘이란?
- 정의: 모델이 출력 단어를 예측할 때, 입력 문장의 모든 단어를 동일하게 참고하는 것이 아니라 관련이 있는 특정 단어에 더 집중(Attention)하여 정보를 처리하는 기법입니다.
- 등장 배경: 기존 Seq2Seq 모델은 입력 문장을 하나의 고정된 벡터(컨텍스트 벡터)로 압축하면서 정보 손실이 발생하는 '병목 현상'과 긴 문장에서 성능이 떨어지는 문제를 해결하기 위해 도입되었습니다.

2. 주요 활용 사례
- 감성 분석(Sentiment Analysis): 리뷰 문장에서 긍정/부정 판단에 결정적인 영향을 주는 핵심 단어(예: '존잼', '실망')에 집중하여 예측력을 높입니다.
- 기계 번역(Machine Translation): 출력할 단어와 연관성이 높은 입력 문장의 특정 단어들을 매칭(Alignment)하여 더 정확한 번역을 수행합니다.

3. 어텐션의 기본 원리 (Attention Function)
- 구조: $Attention(Q, K, V) = Attention Value 
  - Query (Q): 현재 시점의 디코더 셀의 은닉 상태 (무엇을 찾고자 하는가?) 
  - Keys (K): 모든 시점의 인코더 셀의 은닉 상태들 (대상 데이터의 이름표) 
  - Values (V): 모든 시점의 인코더 셀의 은닉 상태들 (대상 데이터의 실제 값) 
- 작동 방식: Query와 모든 Key 사이의 유사도를 구하고, 이 유사도를 가중치로 삼아 Value들을 모두 더하여 '어텐션 값(Attention Value)'을 산출합니다.

4. 주요 어텐션 종류 및 차이점
- 어텐션 메커니즘은 어텐션 스코어(Score) 함수를 정의하는 방식에 따라 구분됩니다.
  1) 바다나우 어텐션 (Bahdanau Attention) 
    - 초기 어텐션 모델로, 컨캣(Concat) 어텐션이라고도 불립니다.
    - 디코더의 이전 시점 은닉 상태($s_{t-1}$)와 인코더의 은닉 상태($h_i$)를 사용하여 계산합니다.
    - score(s_{t}, h_{i}) = W_{a}^{T} \tanh(W_{b}[s_{t}; h_{i}])
  2) 루옹 어텐션 (Luong Attention) 바다나우 방식보다 계산 효율을 높인 방식입니다.
    - 주로 **Dot-Product(내적)**를 사용하여 스코어를 계산하며, 제네럴(General) 어텐션 등 변형이 존재합니다.
    - score(s_{t}, h_{i}) = s_{t}^{T} h_{i}$ (Dot-Product 기준) 

5. 느낀점
- 고정된 크기의 벡터에 정보를 구겨 넣으려던 기존 방식의 한계를 필요할때마다 원문을 다시 들여다보는 어텐션 방식으로 전환하여 해결했다는 점이 공학적으로 유연한 사고라고 느껴짐.
- 악성 코드 탐지 시스템에 어텐션이 적용될 경우 공격자는 시스템이 중요하게 여기는 키워드를 우회하거나 어텐션을 교란하는 방식으로 방어 체계를 무너뜨릴 수 있다는 점이 보안적으로 해결해야 할 숙제처럼 보여짐.
- 보안 관점에서는 모델의 취약점을 분석하는 지표가 될수도 있고 반대로 이상 징후를 잡아내는 방어 기제가 될 수수도 있다는 양면성을 깊이 고민하는 시간이 됨.
