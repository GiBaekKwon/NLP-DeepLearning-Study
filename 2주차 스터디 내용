[Week 02] Word Embedding (워드 임베딩)
### 📅 스터디 일시
- 2026년 2월 5일(목) 10:00~12:00

### 📅 스터디 주제
- **워드 임베딩(Word Embedding)의 개념과 주요 알고리즘**
- **희소 표현(Sparse Representation)에서 밀집 표현(Dense Representation)으로의 전환**

### 🎯 학습 목표
1. 단어를 고차원의 희소 벡터가 아닌 저차원의 밀집 벡터로 변환하는 원리 이해
2. Word2Vec, FastText, GloVe 등 대표적인 임베딩 기법의 특징 및 차이점 학습
3. 사전 훈련된 임베딩(Pre-trained Embedding)을 활용한 전이 학습 이해

### 📝 주요 학습 내용
#### 1. 워드 임베딩 (Word Embedding)
- 개념: 단어를 컴퓨터가 처리할 수 있는 저차원의 실수 벡터로 매핑하는 기술.
- 차별점: 원-핫 인코딩과 달리 단어 간의 유사도(Similarity)를 계산할 수 있으며, 데이터의 차원을 획기적으로 줄여 차원의 저주 문제를 해결함.

#### 2. 대표적인 임베딩 알고리즘
- Word2Vec: 주변 단어의 문맥을 통해 단어의 의미를 학습 (CBOW, Skip-gram). 단어 간의 연산(예: 한국 - 서울 + 도쿄 = 일본)이 가능함.
- FastText: 단어를 철자(Character) 단위의 n-gram으로 쪼개어 학습. 사전에 없는 단어(OOV, Out-of-Vocabulary)나 오타가 포함된 텍스트 처리에 강점이 있음.
- GloVe: 단어 간의 동시 등장 확률(Co-occurrence) 통계를 활용하여 코퍼스 전체의 전역적 정보를 반영함.

#### 3. 임베딩 층 (Embedding Layer) & 실습
- Embedding Layer: 훈련 데이터로부터 모델이 직접 임베딩 벡터를 학습하도록 설계된 층.
- 전이 학습: 이미 대량의 말뭉치로 학습된 Word2Vec이나 GloVe 가중치를 가져와 내 모델에 적용함으로써 적은 데이터로도 높은 성능을 내는 방법 학습.

💡 느낀 점
- SSH 접속 시 패스워드 대신에 키를 사용하는 것처럼 자연어 처리에서도 단순 텍스트를 고유한 벡터값으로 치환하여 관리하는 점이 보안의 인증 메커니즘과 비슷한거 같음.
- FastText의 n-gram 방식은 악성코드의 시그니처가 조금 변조되더라도 공통된 패턴을 찾아내는 다향성 악성코드 탐지 기법에 응용하면 좋을것 같음. 
